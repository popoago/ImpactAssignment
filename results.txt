model: Resnet architecture modified for the given dataset. The model is based on fully preactivated resnet architecture. It consists of a convolution layer followed by 3 groups of 9 BasicBlock each, an average pooling layer followed by a linear layer with 200 classes. BasicBlock consists of 2 layers of batchnorm, relu and a conv layer which acts as our residual portion and then adding it to the identity layer to get the final output.
The weights were initialised based on the accepted norms.

hyperparameters: the parameters were mostly chosen based on the ideas of one cycle policy. What we do here is run our model between two learning rates for fixed number of steps and see the losses for different learning rates. We then choose the learning rate where the model's loss starts to increase, normally a tenth of that value. This generally helps in converging the model faster. The weight decay and momentum values were also chosen in a similar manner. According to the same paper, batch_size should also be kept at a maximum value(I tested with 1000), but other sizes of 16 and 32 were also tested on.

for calculating the loss, crossentropy was chosen, which implements the logsoftmax function on the linear layer's output of the model and then calculates the negative loglikelihood loss.
Both the SGD and Adam optimizers were tested, but because of the incompatibility of the cyclic learning rate scheduler(based on Leslie Smith's paper), SGD was chosen for the final architecture.
The accuracy was calculated by comparing the model's ouput to the actual output both for validation as well as the training dataset.

Problems encountered and future modifications:
I hadn't worked with pytorch before so creating and training a model from scrath took a lot of time. While Cifar10 had a lot of literature associated with it, the same wasn't true for tiny-imagenet. Because of limited no. of images per class and relatively huge number of classes, it was difficult to train the model from scratch. I did experiment with different optimizers, hyperparameters and schedulers and tried adding dropout to the resnet model(which didn't really help) but the model was still badly overfitting. Neither a smaller architecture(a ResNet20) nor different data augmentation techniques helped. I'm assuming this result is because of the choice learning rate but I'm not sure. If given more time, I would have experimented with maybe a simpler architecture(say a VGG) or a densnet/wideresnet to ascertain if it's a hyper parameter problem or an architecture problem but as of right now, I'm publishing the test results as is. I also wanted to try the training on fasai library which has implemented AdamW and one cycle policy for really fast convergence so that is something I would have experimented with as well.

